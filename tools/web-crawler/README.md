# æ™ºèƒ½ç½‘ç«™çˆ¬è™«å·¥å…·

åŸºäº Crawl4AI å®ç°çš„ç½‘ç«™ç»“æ„å‘ç°å’Œå†…å®¹æ‰¹é‡æŠ“å–çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œæä¾›å‘½ä»¤è¡Œç‰ˆæœ¬å’ŒWebå¯è§†åŒ–ç‰ˆæœ¬ã€‚

## ğŸŒŸ åŠŸèƒ½ç‰¹æ€§

### ğŸ” æ™ºèƒ½ç½‘ç«™å‘ç°
- **è‡ªåŠ¨ç»“æ„è¯†åˆ«** - æ™ºèƒ½å‘ç°ç½‘ç«™çš„æ‰€æœ‰å­é¡µé¢å’Œå¯¼èˆªç»“æ„
- **æ·±åº¦çˆ¬å–ç­–ç•¥** - æ”¯æŒå¹¿åº¦ä¼˜å…ˆ(BFS)å’Œæ·±åº¦ä¼˜å…ˆ(DFS)ç­–ç•¥
- **æ™ºèƒ½é“¾æ¥è¿‡æ»¤** - è‡ªåŠ¨è¿‡æ»¤æ— æ•ˆã€é‡å¤å’Œå¤–éƒ¨é“¾æ¥
- **å¯¼èˆªé¡ºåºæŠ“å–** - æŒ‰ç…§ç½‘ç«™å¯¼èˆªç»“æ„æœ‰åºæŠ“å–å†…å®¹

### ğŸ“„ å†…å®¹æ‰¹é‡æŠ“å–
- **é«˜æ•ˆå¹¶å‘å¤„ç†** - æ”¯æŒæ‰¹é‡å¹¶å‘æŠ“å–ï¼Œå¯é…ç½®å¹¶å‘æ•°
- **å¤šç§æå–ç­–ç•¥** - CSSé€‰æ‹©å™¨ã€XPathã€LLMæ™ºèƒ½æå–ã€æ­£åˆ™è¡¨è¾¾å¼
- **ç»“æ„åŒ–æ•°æ®** - è‡ªåŠ¨æå–æ ‡é¢˜ã€æè¿°ã€æ­£æ–‡ã€é“¾æ¥ç­‰ç»“æ„åŒ–ä¿¡æ¯
- **å¤šæ ¼å¼è¾“å‡º** - Markdownã€JSONã€HTMLç´¢å¼•ã€æˆªå›¾ã€PDF

### ğŸ›ï¸ å¯è§†åŒ–Webç•Œé¢
- **ç°ä»£åŒ–UIè®¾è®¡** - å“åº”å¼è®¾è®¡ï¼Œæ”¯æŒç§»åŠ¨ç«¯è®¿é—®
- **å®æ—¶è¿›åº¦æ˜¾ç¤º** - WebSocketå®æ—¶é€šä¿¡ï¼Œæ˜¾ç¤ºæŠ“å–è¿›åº¦å’Œæ—¥å¿—
- **å¯è§†åŒ–é…ç½®** - å›¾å½¢åŒ–é…ç½®æ‰€æœ‰å‚æ•°ï¼Œæ— éœ€ç¼–ç¨‹çŸ¥è¯†
- **ç»“æœå¯è§†åŒ–** - æ ‘å½¢å¯¼èˆªç»“æ„ã€æœç´¢è¿‡æ»¤ã€è¯¦æƒ…æŸ¥çœ‹

### ğŸ“Š æ•°æ®å­˜å‚¨ä¸åˆ†æ
- **å®Œæ•´æ˜ å°„å…³ç³»** - URLä¸å†…å®¹çš„å®Œæ•´æ˜ å°„å’Œç´¢å¼•
- **å¤šç§å­˜å‚¨æ ¼å¼** - JSONç»“æ„åŒ–å­˜å‚¨ã€Markdownæ–‡ä»¶ã€HTMLç´¢å¼•
- **ç»Ÿè®¡åˆ†æ** - è¯¦ç»†çš„æŠ“å–ç»Ÿè®¡å’Œæ€§èƒ½åˆ†æ
- **å¯¼èˆªç»“æ„** - å¯è§†åŒ–ç½‘ç«™å±‚çº§ç»“æ„å’Œé¡µé¢å…³ç³»

## ğŸ“¦ å®‰è£…ä¸ç¯å¢ƒé…ç½®

### ç³»ç»Ÿè¦æ±‚
- Python 3.8+
- Windows/macOS/Linux
- è‡³å°‘ 4GB å†…å­˜
- ç¨³å®šçš„ç½‘ç»œè¿æ¥

### å®‰è£…ä¾èµ–

#### åŸºç¡€å®‰è£…
```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install crawl4ai

# å®‰è£… Playwright æµè§ˆå™¨
playwright install chromium
```

#### Webç‰ˆæœ¬ä¾èµ–
```bash
# è¿›å…¥Webç›®å½•
cd tools/web-crawler/web

# å®‰è£…Webç‰ˆæœ¬ä¾èµ–
pip install -r requirements.txt
```

#### å¯é€‰ä¾èµ–ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
```bash
# LLMæå–ç­–ç•¥
pip install openai anthropic

# å…¶ä»–æ ¼å¼æ”¯æŒ
pip install beautifulsoup4 lxml
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šWebå¯è§†åŒ–ç•Œé¢ï¼ˆæ¨èï¼‰

#### 1. å¯åŠ¨WebæœåŠ¡å™¨
```bash
cd tools/web-crawler/web
python start.py
```

#### 2. æ‰“å¼€æµè§ˆå™¨
è®¿é—®ï¼š`http://localhost:5000`

#### 3. é…ç½®æŠ“å–å‚æ•°
- **ç›®æ ‡URL**ï¼šè¾“å…¥è¦æŠ“å–çš„ç½‘ç«™åœ°å€
- **çˆ¬å–æ·±åº¦**ï¼šè®¾ç½®æŠ“å–å±‚çº§ï¼ˆ1-5ï¼‰
- **æœ€å¤§é¡µé¢**ï¼šé™åˆ¶æŠ“å–çš„é¡µé¢æ•°é‡
- **æ‰¹å¤„ç†å¤§å°**ï¼šå¹¶å‘æŠ“å–çš„é¡µé¢æ•°
- **çˆ¬å–ç­–ç•¥**ï¼šé€‰æ‹©BFSæˆ–DFSç­–ç•¥

#### 4. å¼€å§‹æŠ“å–
ç‚¹å‡»"å¼€å§‹æŠ“å–"æŒ‰é’®ï¼Œå®æ—¶æŸ¥çœ‹ï¼š
- ğŸ“Š æŠ“å–è¿›åº¦å’Œç»Ÿè®¡
- ğŸ“ è¯¦ç»†çš„æ“ä½œæ—¥å¿—
- ğŸ—‚ï¸ æŠ“å–çš„é¡µé¢åˆ—è¡¨
- ğŸŒ³ ç½‘ç«™å¯¼èˆªç»“æ„

### æ–¹å¼äºŒï¼šå‘½ä»¤è¡Œç‰ˆæœ¬ï¼ˆé«˜çº§ç”¨æˆ·ï¼‰

#### 1. ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬
```bash
# ç¼–è¾‘é…ç½®æ–‡ä»¶
vim website_crawler.py

# è¿è¡Œçˆ¬è™«
python website_crawler.py
```

#### 2. å¿«é€Ÿæµ‹è¯•
```bash
# ä½¿ç”¨é¢„è®¾é…ç½®æµ‹è¯•
python test_cursor_docs.py
```

## âš™ï¸ è¯¦ç»†é…ç½®è¯´æ˜

### Webç•Œé¢é…ç½®é€‰é¡¹

#### åŸºç¡€è®¾ç½®
| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | ç¤ºä¾‹ |
|------|------|--------|------|
| ç›®æ ‡URL | è¦æŠ“å–çš„ç½‘ç«™åœ°å€ | æ—  | `https://docs.cursor.com/welcome` |
| è¾“å‡ºç›®å½• | ç»“æœä¿å­˜ç›®å½• | `crawl_results` | `my_crawl_data` |
| çˆ¬å–æ·±åº¦ | æœ€å¤§æŠ“å–å±‚çº§ | `3` | `1-5` |
| æœ€å¤§é¡µé¢ | é™åˆ¶æŠ“å–é¡µé¢æ•° | `50` | `10-1000` |
| æ‰¹å¤„ç†å¤§å° | å¹¶å‘æŠ“å–æ•°é‡ | `5` | `1-20` |

#### çˆ¬å–ç­–ç•¥
| ç­–ç•¥ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **BFS (å¹¿åº¦ä¼˜å…ˆ)** | é€å±‚æŠ“å–ï¼Œå…ˆæŠ“å–åŒçº§é¡µé¢ | å¤§å‹ç½‘ç«™ã€å®Œæ•´æŠ“å– |
| **DFS (æ·±åº¦ä¼˜å…ˆ)** | æ·±å…¥æŠ“å–ï¼Œæ²¿è·¯å¾„æ·±å…¥ | ç‰¹å®šè·¯å¾„ã€å¿«é€Ÿé¢„è§ˆ |
| **é¦–é¡µé“¾æ¥** | ä»…æŠ“å–é¦–é¡µå‘ç°çš„é“¾æ¥ | å¿«é€Ÿæµ‹è¯•ã€å°è§„æ¨¡æŠ“å– |

#### ç¼“å­˜æ¨¡å¼
| æ¨¡å¼ | è¯´æ˜ | ä½¿ç”¨åœºæ™¯ |
|------|------|----------|
| `BYPASS` | ç»•è¿‡ç¼“å­˜ï¼Œæ€»æ˜¯é‡æ–°æŠ“å– | å®æ—¶æ•°æ®ã€æµ‹è¯• |
| `ENABLED` | å¯ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤æŠ“å– | å¤§è§„æ¨¡æŠ“å–ã€èŠ‚çœèµ„æº |
| `READ_ONLY` | åªè¯»ç¼“å­˜ï¼Œä¸æ›´æ–°ç¼“å­˜ | ç¦»çº¿åˆ†æ |

#### è¾“å‡ºæ ¼å¼
- âœ… **Markdown** - æ¸…æ´çš„æ–‡æœ¬æ ¼å¼ï¼Œé€‚åˆé˜…è¯»
- âœ… **JSON** - ç»“æ„åŒ–æ•°æ®ï¼Œé€‚åˆç¨‹åºå¤„ç†
- âœ… **HTMLç´¢å¼•** - å¯è§†åŒ–æµè§ˆç•Œé¢
- âœ… **æˆªå›¾** - é¡µé¢æˆªå›¾ï¼ˆå¯é€‰ï¼‰
- âœ… **PDF** - PDFæ ¼å¼ä¿å­˜ï¼ˆå¯é€‰ï¼‰

#### è¿‡æ»¤é€‰é¡¹
| é€‰é¡¹ | è¯´æ˜ | æ¨èè®¾ç½® |
|------|------|----------|
| æ’é™¤å¤–éƒ¨é“¾æ¥ | ä¸æŠ“å–å…¶ä»–ç½‘ç«™çš„é“¾æ¥ | âœ… å¯ç”¨ |
| æ’é™¤ç¤¾äº¤åª’ä½“ | è·³è¿‡ç¤¾äº¤åª’ä½“é“¾æ¥ | âœ… å¯ç”¨ |
| æ’é™¤å›¾ç‰‡é“¾æ¥ | ä¸å¤„ç†å›¾ç‰‡æ–‡ä»¶é“¾æ¥ | âœ… å¯ç”¨ |
| å¤„ç†iframe | æŠ“å–åµŒå…¥å¼æ¡†æ¶å†…å®¹ | âŒ ç¦ç”¨ |

#### æµè§ˆå™¨é…ç½®
| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| æµè§ˆå™¨ç±»å‹ | Chromium/Firefox/WebKit | `Chromium` |
| æ— å¤´æ¨¡å¼ | åå°è¿è¡Œï¼Œä¸æ˜¾ç¤ºç•Œé¢ | âœ… å¯ç”¨ |
| è¯¦ç»†æ—¥å¿— | è¾“å‡ºè¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ | âŒ ç¦ç”¨ |
| User Agent | æµè§ˆå™¨æ ‡è¯†å­—ç¬¦ä¸² | ä½¿ç”¨é»˜è®¤ |
| ä»£ç†æœåŠ¡å™¨ | HTTPä»£ç†è®¾ç½® | æŒ‰éœ€é…ç½® |

#### æå–ç­–ç•¥
| ç­–ç•¥ | è¯´æ˜ | é…ç½®è¦æ±‚ |
|------|------|----------|
| **CSSé€‰æ‹©å™¨** | ä½¿ç”¨CSSé€‰æ‹©å™¨æå–å†…å®¹ | éœ€è¦JSONé…ç½® |
| **XPath** | ä½¿ç”¨XPathè¡¨è¾¾å¼ | éœ€è¦XPathè¯­æ³• |
| **LLMæ™ºèƒ½æå–** | ä½¿ç”¨AIæ¨¡å‹æ™ºèƒ½æå– | éœ€è¦APIå¯†é’¥ |
| **æ­£åˆ™è¡¨è¾¾å¼** | ä½¿ç”¨æ­£åˆ™åŒ¹é…å†…å®¹ | éœ€è¦æ­£åˆ™è¡¨è¾¾å¼ |

### å‘½ä»¤è¡Œé…ç½®ï¼ˆé«˜çº§ï¼‰

```python
# åŸºç¡€é…ç½®
config = {
    "target_url": "https://docs.cursor.com/welcome",
    "output_dir": "crawl_results",
    "max_depth": 3,
    "max_pages": 50,
    "batch_size": 5,
    "crawl_strategy": "bfs",  # bfs, dfs, links_only
    "cache_mode": "BYPASS",
    "word_threshold": 20
}

# æµè§ˆå™¨é…ç½®
browser_config = BrowserConfig(
    headless=True,
    verbose=False,
    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    proxy=None
)

# è¿‡æ»¤å™¨é…ç½®
filters = {
    "exclude_external": True,
    "exclude_social": True,
    "exclude_images": True,
    "process_iframes": False,
    "exclude_domains": ["ads.example.com"],
    "exclude_patterns": ["*.pdf", "*.jpg", "*.png"]
}
```

## ğŸ“ è¾“å‡ºç»“æœä¸æ–‡ä»¶ç»“æ„

### Webç‰ˆæœ¬è¾“å‡º
Webç‰ˆæœ¬ä¼šåœ¨æŒ‡å®šç›®å½•ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ç»“æ„ï¼š

```
crawl_results/
â”œâ”€â”€ index.html                 # ğŸ“Š å¯è§†åŒ–ç´¢å¼•é¡µé¢ï¼Œæ”¯æŒæœç´¢å’Œç­›é€‰
â”œâ”€â”€ discovered_urls.json       # ğŸ”— å‘ç°çš„URLåˆ—è¡¨ï¼ˆJSONæ ¼å¼ï¼‰
â”œâ”€â”€ discovered_urls.txt        # ğŸ“ å‘ç°çš„URLåˆ—è¡¨ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
â”œâ”€â”€ crawled_content.json       # ğŸ“¦ æ‰€æœ‰æŠ“å–å†…å®¹çš„å®Œæ•´æ•°æ®
â”œâ”€â”€ task_info.json            # â„¹ï¸ ä»»åŠ¡é…ç½®å’Œç»Ÿè®¡ä¿¡æ¯
â””â”€â”€ individual_pages/         # ğŸ“„ å•ä¸ªé¡µé¢å†…å®¹ï¼ˆæŒ‰éœ€ç”Ÿæˆï¼‰
    â”œâ”€â”€ page_001.json
    â”œâ”€â”€ page_002.json
    â””â”€â”€ ...
```

### å‘½ä»¤è¡Œç‰ˆæœ¬è¾“å‡º
å‘½ä»¤è¡Œç‰ˆæœ¬æä¾›æ›´è¯¦ç»†çš„æ–‡ä»¶ç»„ç»‡ï¼š

```
crawl_results/
â”œâ”€â”€ index.html                 # ğŸ“Š å¯è§†åŒ–ç´¢å¼•é¡µé¢
â”œâ”€â”€ discovered_urls.json       # ğŸ”— å‘ç°çš„URLåˆ—è¡¨ï¼ˆJSONï¼‰
â”œâ”€â”€ discovered_urls.txt        # ğŸ“ å‘ç°çš„URLåˆ—è¡¨ï¼ˆæ–‡æœ¬ï¼‰
â”œâ”€â”€ crawled_content.json       # ğŸ“¦ æ‰€æœ‰æŠ“å–å†…å®¹ï¼ˆJSONï¼‰
â”œâ”€â”€ url_mapping.json          # ğŸ—ºï¸ URLæ˜ å°„å…³ç³»
â”œâ”€â”€ crawl_stats.json          # ğŸ“ˆ è¯¦ç»†çš„æŠ“å–ç»Ÿè®¡ä¿¡æ¯
â”œâ”€â”€ content/                  # ğŸ“„ å•ä¸ªé¡µé¢å†…å®¹ï¼ˆJSONæ ¼å¼ï¼‰
â”‚   â”œâ”€â”€ page1.json
â”‚   â”œâ”€â”€ page2.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ markdown/                 # ğŸ“ å•ä¸ªé¡µé¢å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰
â”‚   â”œâ”€â”€ page1.md
â”‚   â”œâ”€â”€ page2.md
â”‚   â””â”€â”€ ...
â””â”€â”€ screenshots/              # ğŸ“¸ é¡µé¢æˆªå›¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    â”œâ”€â”€ page1.png
    â”œâ”€â”€ page2.png
    â””â”€â”€ ...
```

### æ•°æ®æ ¼å¼è¯´æ˜

#### crawled_content.json ç»“æ„
```json
{
  "https://example.com/page1": {
    "url": "https://example.com/page1",
    "title": "é¡µé¢æ ‡é¢˜",
    "description": "é¡µé¢æè¿°",
    "timestamp": "2025-01-11T10:30:00",
    "status_code": 200,
    "markdown": "# é¡µé¢æ ‡é¢˜\n\né¡µé¢å†…å®¹...",
    "links_count": 25,
    "extracted_content": {
      "title": "æå–çš„æ ‡é¢˜",
      "main_content": "ä¸»è¦å†…å®¹",
      "headings": ["æ ‡é¢˜1", "æ ‡é¢˜2"],
      "images": ["image1.jpg", "image2.png"]
    }
  }
}
```

#### ç»Ÿè®¡ä¿¡æ¯æ ¼å¼
```json
{
  "task_id": "uuid-string",
  "start_time": "2025-01-11T10:30:00",
  "end_time": "2025-01-11T10:35:00",
  "duration_seconds": 300,
  "total_discovered": 50,
  "total_crawled": 45,
  "total_failed": 5,
  "success_rate": 90.0,
  "pages_per_second": 0.15,
  "config": {
    "target_url": "https://example.com",
    "max_depth": 3,
    "strategy": "bfs"
  }
}
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹ä¸æœ€ä½³å®è·µ

### ç¤ºä¾‹1ï¼šWebç•Œé¢å¿«é€Ÿä¸Šæ‰‹

#### æŠ“å–æŠ€æœ¯æ–‡æ¡£ç½‘ç«™
1. **å¯åŠ¨WebæœåŠ¡**
   ```bash
   cd tools/web-crawler/web
   python start.py
   ```

2. **é…ç½®å‚æ•°**
   - ç›®æ ‡URL: `https://docs.cursor.com/welcome`
   - çˆ¬å–æ·±åº¦: `2`
   - æœ€å¤§é¡µé¢: `30`
   - ç­–ç•¥: `BFSå¹¿åº¦ä¼˜å…ˆ`
   - è¾“å‡ºæ ¼å¼: `Markdown + JSON + HTMLç´¢å¼•`

3. **å®æ—¶ç›‘æ§**
   - æŸ¥çœ‹è¿›åº¦æ¡å’Œç»Ÿè®¡ä¿¡æ¯
   - ç›‘æ§å®æ—¶æ—¥å¿—è¾“å‡º
   - è§‚å¯Ÿé¡µé¢å‘ç°è¿‡ç¨‹

#### æŠ“å–æ–°é—»ç½‘ç«™
1. **é…ç½®å‚æ•°**
   - ç›®æ ‡URL: `https://news.example.com`
   - çˆ¬å–æ·±åº¦: `1` (ä»…é¦–é¡µé“¾æ¥)
   - è¿‡æ»¤è®¾ç½®: æ’é™¤å¤–éƒ¨é“¾æ¥ã€ç¤¾äº¤åª’ä½“
   - æå–ç­–ç•¥: CSSé€‰æ‹©å™¨

2. **CSSé€‰æ‹©å™¨é…ç½®**
   ```json
   {
     "name": "NewsContent",
     "baseSelector": "article",
     "fields": [
       {"name": "title", "selector": "h1, .title", "type": "text"},
       {"name": "author", "selector": ".author", "type": "text"},
       {"name": "date", "selector": ".date", "type": "text"},
       {"name": "content", "selector": ".content, .article-body", "type": "text"}
     ]
   }
   ```

### ç¤ºä¾‹2ï¼šå‘½ä»¤è¡Œé«˜çº§é…ç½®

#### ä¼ä¸šç½‘ç«™å®Œæ•´æŠ“å–
```python
import asyncio
from website_crawler import WebsiteCrawler

async def crawl_enterprise_site():
    config = {
        "target_url": "https://company.example.com",
        "output_dir": "enterprise_crawl",
        "max_depth": 4,
        "max_pages": 200,
        "batch_size": 8,
        "crawl_strategy": "bfs",
        "filters": {
            "exclude_external": True,
            "exclude_patterns": ["*.pdf", "*.doc", "*.zip"],
            "exclude_domains": ["ads.example.com", "tracking.example.com"]
        }
    }
    
    crawler = WebsiteCrawler(config)
    results = await crawler.run()
    
    print(f"æŠ“å–å®Œæˆ:")
    print(f"- å‘ç°é¡µé¢: {results['discovered']}")
    print(f"- æˆåŠŸæŠ“å–: {results['successful']}")
    print(f"- å¤±è´¥é¡µé¢: {results['failed']}")

asyncio.run(crawl_enterprise_site())
```

#### ç”µå•†äº§å“é¡µé¢æŠ“å–
```python
# ä¸“é—¨é’ˆå¯¹äº§å“é¡µé¢çš„æå–ç­–ç•¥
product_extraction = {
    "name": "ProductInfo",
    "baseSelector": ".product-detail",
    "fields": [
        {"name": "product_name", "selector": "h1.product-title", "type": "text"},
        {"name": "price", "selector": ".price-current", "type": "text"},
        {"name": "description", "selector": ".product-description", "type": "text"},
        {"name": "images", "selector": ".product-images img", "type": "attribute", "attribute": "src"},
        {"name": "specifications", "selector": ".spec-table tr", "type": "text"},
        {"name": "reviews_count", "selector": ".reviews-count", "type": "text"}
    ]
}
```

### ç¤ºä¾‹3ï¼šLLMæ™ºèƒ½æå–

#### OpenAI GPTæå–
åœ¨Webç•Œé¢ä¸­é…ç½®LLMæå–:
- **æå–ç±»å‹**: LLMæ™ºèƒ½æå–
- **LLMæä¾›å•†**: `openai/gpt-4o-mini`
- **APIå¯†é’¥**: è¾“å…¥æ‚¨çš„OpenAI APIå¯†é’¥
- **æå–æŒ‡ä»¤**: 
  ```
  è¯·æå–é¡µé¢ä¸­çš„ä»¥ä¸‹ä¿¡æ¯ï¼š
  1. ä¸»è¦æ ‡é¢˜å’Œå‰¯æ ‡é¢˜
  2. æ–‡ç« æ‘˜è¦æˆ–äº§å“æè¿°
  3. å…³é”®æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯
  4. è”ç³»æ–¹å¼æˆ–é‡è¦é“¾æ¥
  è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœ
  ```

#### Anthropic Claudeæå–
```python
# å‘½ä»¤è¡Œç‰ˆæœ¬çš„LLMé…ç½®
llm_config = {
    "provider": "anthropic/claude-3-sonnet",
    "api_token": "your-anthropic-api-key",
    "instruction": """
    åˆ†æé¡µé¢å†…å®¹å¹¶æå–ï¼š
    - æ ¸å¿ƒä¿¡æ¯å’Œè¦ç‚¹
    - æŠ€æœ¯è§„æ ¼æˆ–å‚æ•°
    - ç”¨æˆ·è¯„ä»·æˆ–åé¦ˆ
    - ç›¸å…³é“¾æ¥å’Œèµ„æº
    
    è¿”å›ç»“æ„åŒ–çš„JSONæ•°æ®
    """,
    "extraction_type": "json"
}
```

### ç¤ºä¾‹4ï¼šæ‰¹é‡å¤„ç†å¤šä¸ªç½‘ç«™

#### åˆ›å»ºæ‰¹é‡ä»»åŠ¡è„šæœ¬
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

websites = [
    {"url": "https://docs.site1.com", "name": "site1_docs"},
    {"url": "https://help.site2.com", "name": "site2_help"},
    {"url": "https://guide.site3.com", "name": "site3_guide"}
]

async def batch_crawl():
    tasks = []
    for site in websites:
        config = {
            "target_url": site["url"],
            "output_dir": f"batch_results/{site['name']}",
            "max_depth": 2,
            "max_pages": 50
        }
        tasks.append(crawl_single_site(config))
    
    results = await asyncio.gather(*tasks)
    return results

async def crawl_single_site(config):
    # å®ç°å•ä¸ªç½‘ç«™çš„æŠ“å–é€»è¾‘
    pass
```

## â“ å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### Webç•Œé¢å¸¸è§é—®é¢˜

#### Q: WebæœåŠ¡å¯åŠ¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ
**A: æŒ‰ä»¥ä¸‹æ­¥éª¤æ’æŸ¥ï¼š**
1. **æ£€æŸ¥ä¾èµ–å®‰è£…**
   ```bash
   cd tools/web-crawler/web
   pip install -r requirements.txt
   ```

2. **æ£€æŸ¥ç«¯å£å ç”¨**
   ```bash
   # Windows
   netstat -ano | findstr :5000
   
   # Linux/Mac
   lsof -i :5000
   ```

3. **æŸ¥çœ‹é”™è¯¯æ—¥å¿—**
   ```bash
   python start.py
   # æŸ¥çœ‹æ§åˆ¶å°è¾“å‡ºçš„é”™è¯¯ä¿¡æ¯
   ```

#### Q: WebSocketè¿æ¥å¤±è´¥ï¼Ÿ
**A: å¸¸è§è§£å†³æ–¹æ¡ˆï¼š**
- ç¡®ä¿é˜²ç«å¢™å…è®¸5000ç«¯å£
- æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦é˜»æ­¢WebSocketè¿æ¥
- å°è¯•ä½¿ç”¨ `http://127.0.0.1:5000` è€Œä¸æ˜¯ `localhost`

#### Q: æŠ“å–ä»»åŠ¡å¡ä½ä¸åŠ¨ï¼Ÿ
**A: å¯èƒ½çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š**
1. **ç›®æ ‡ç½‘ç«™å“åº”æ…¢** - å¢åŠ å»¶è¿Ÿæ—¶é—´
2. **ç½‘ç»œè¿æ¥é—®é¢˜** - æ£€æŸ¥ç½‘ç»œçŠ¶æ€
3. **åçˆ¬è™«æœºåˆ¶** - é™ä½å¹¶å‘æ•°ï¼Œå¢åŠ å»¶è¿Ÿ
4. **å†…å­˜ä¸è¶³** - å‡å°‘æ‰¹å¤„ç†å¤§å°

### æŠ“å–ç­–ç•¥é—®é¢˜

#### Q: å¦‚ä½•å¤„ç†éœ€è¦ç™»å½•çš„ç½‘ç«™ï¼Ÿ
**A: å¤šç§è®¤è¯æ–¹å¼ï¼š**

**æ–¹å¼1: åœ¨Webç•Œé¢é…ç½®Cookies**
- å…ˆåœ¨æµè§ˆå™¨ç™»å½•ç›®æ ‡ç½‘ç«™
- ä½¿ç”¨å¼€å‘è€…å·¥å…·å¤åˆ¶Cookie
- åœ¨é«˜çº§é€‰é¡¹ä¸­é…ç½®Cookieå­—ç¬¦ä¸²

**æ–¹å¼2: å‘½ä»¤è¡Œç‰ˆæœ¬é…ç½®**
```python
browser_config = BrowserConfig(
    headless=True,
    cookies=[
        {"name": "session_id", "value": "your_session", "domain": "example.com"},
        {"name": "auth_token", "value": "token_value", "domain": "example.com"}
    ]
)
```

#### Q: å¦‚ä½•é¿å…è¢«ç½‘ç«™å°ç¦ï¼Ÿ
**A: é‡‡ç”¨å‹å¥½çš„çˆ¬å–ç­–ç•¥ï¼š**

**Webç•Œé¢è®¾ç½®ï¼š**
- æ‰¹å¤„ç†å¤§å°ï¼šè®¾ç½®ä¸º 1-3
- å»¶è¿Ÿæ—¶é—´ï¼šè®¾ç½®ä¸º 2-5 ç§’
- å¯ç”¨ä»£ç†æœåŠ¡å™¨
- ä½¿ç”¨çœŸå®çš„User-Agent

**å‘½ä»¤è¡Œé«˜çº§é…ç½®ï¼š**
```python
# 1. æ§åˆ¶å¹¶å‘å’Œå»¶è¿Ÿ
config = {
    "batch_size": 2,
    "delay": 3.0,
    "max_pages": 20  # é™åˆ¶æ€»é¡µé¢æ•°
}

# 2. ä½¿ç”¨ä»£ç†è½®æ¢
proxies = [
    "http://proxy1:port",
    "http://proxy2:port",
    "http://proxy3:port"
]

# 3. éšæœºUser-Agent
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36"
]
```

#### Q: å¦‚ä½•å¤„ç†JavaScriptåŠ¨æ€å†…å®¹ï¼Ÿ
**A: é…ç½®ç­‰å¾…æ¡ä»¶ï¼š**

**Webç•Œé¢é…ç½®ï¼š**
- ç­‰å¾…æ¡ä»¶ï¼š`css:.content-loaded`
- å»¶è¿Ÿæ—¶é—´ï¼šå¢åŠ åˆ° 3-5 ç§’
- å¯ç”¨è¯¦ç»†æ—¥å¿—æŸ¥çœ‹åŠ è½½è¿‡ç¨‹

**å‘½ä»¤è¡Œé…ç½®ï¼š**
```python
run_config = CrawlerRunConfig(
    wait_for="css:.content-loaded",  # ç­‰å¾…ç‰¹å®šå…ƒç´ 
    # æˆ–è€…
    wait_for="js:window.dataReady === true",  # ç­‰å¾…JSæ¡ä»¶
    # æˆ–è€…
    wait_for="networkidle",  # ç­‰å¾…ç½‘ç»œç©ºé—²
    delay=3.0  # é¢å¤–å»¶è¿Ÿ
)
```

### å†…å®¹æå–é—®é¢˜

#### Q: å¦‚ä½•æå–ç‰¹å®šæ ¼å¼çš„æ•°æ®ï¼Ÿ
**A: ä½¿ç”¨åˆé€‚çš„æå–ç­–ç•¥ï¼š**

**æ–°é—»æ–‡ç« æå–ï¼š**
```json
{
  "name": "NewsArticle",
  "baseSelector": "article, .post, .entry",
  "fields": [
    {"name": "headline", "selector": "h1, .headline, .title", "type": "text"},
    {"name": "author", "selector": ".author, .byline", "type": "text"},
    {"name": "publish_date", "selector": ".date, time", "type": "text"},
    {"name": "content", "selector": ".content, .article-body, .post-content", "type": "text"},
    {"name": "tags", "selector": ".tags a, .categories a", "type": "text"}
  ]
}
```

**ç”µå•†äº§å“æå–ï¼š**
```json
{
  "name": "Product",
  "baseSelector": ".product, .item",
  "fields": [
    {"name": "title", "selector": "h1, .product-title", "type": "text"},
    {"name": "price", "selector": ".price, .cost", "type": "text"},
    {"name": "rating", "selector": ".rating, .stars", "type": "text"},
    {"name": "availability", "selector": ".stock, .availability", "type": "text"},
    {"name": "images", "selector": ".product-images img", "type": "attribute", "attribute": "src"}
  ]
}
```

#### Q: LLMæå–æ•ˆæœä¸å¥½æ€ä¹ˆåŠï¼Ÿ
**A: ä¼˜åŒ–æå–æŒ‡ä»¤ï¼š**

**æ”¹è¿›å‰ï¼ˆæ¨¡ç³ŠæŒ‡ä»¤ï¼‰ï¼š**
```
æå–é¡µé¢å†…å®¹
```

**æ”¹è¿›åï¼ˆå…·ä½“æŒ‡ä»¤ï¼‰ï¼š**
```
è¯·åˆ†æè¿™ä¸ªç½‘é¡µå¹¶æå–ä»¥ä¸‹ç»“æ„åŒ–ä¿¡æ¯ï¼š

1. æ–‡æ¡£æ ‡é¢˜å’Œä¸»è¦ç« èŠ‚æ ‡é¢˜
2. æ¯ä¸ªç« èŠ‚çš„æ ¸å¿ƒå†…å®¹æ‘˜è¦ï¼ˆä¸è¶…è¿‡100å­—ï¼‰
3. ä»£ç ç¤ºä¾‹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
4. é‡è¦çš„é“¾æ¥å’Œå¼•ç”¨
5. å…³é”®æœ¯è¯­å’Œå®šä¹‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- title: ä¸»æ ‡é¢˜
- sections: ç« èŠ‚æ•°ç»„ï¼Œæ¯ä¸ªåŒ…å«titleå’Œsummary
- code_examples: ä»£ç ç¤ºä¾‹æ•°ç»„
- links: é‡è¦é“¾æ¥æ•°ç»„
- terms: å…³é”®æœ¯è¯­å¯¹è±¡
```

### æ€§èƒ½ä¼˜åŒ–é—®é¢˜

#### Q: æŠ“å–é€Ÿåº¦å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ
**A: å¤šç§ä¼˜åŒ–ç­–ç•¥ï¼š**

1. **è°ƒæ•´å¹¶å‘å‚æ•°**
   - å¢åŠ æ‰¹å¤„ç†å¤§å°ï¼ˆ8-15ï¼‰
   - å‡å°‘å»¶è¿Ÿæ—¶é—´ï¼ˆ0.5-1ç§’ï¼‰
   - ä½¿ç”¨æ›´å¿«çš„ç½‘ç»œç¯å¢ƒ

2. **é€‰æ‹©åˆé€‚çš„ç­–ç•¥**
   - BFSç­–ç•¥ï¼šé€‚åˆå®Œæ•´æŠ“å–
   - é¦–é¡µé“¾æ¥ï¼šé€‚åˆå¿«é€Ÿé¢„è§ˆ
   - é™åˆ¶æ·±åº¦ï¼šé¿å…è¿‡æ·±æŠ“å–

3. **å¯ç”¨ç¼“å­˜**
   - ç¼“å­˜æ¨¡å¼ï¼šé€‰æ‹©ENABLED
   - é¿å…é‡å¤æŠ“å–ç›¸åŒé¡µé¢

#### Q: å†…å­˜å ç”¨è¿‡é«˜æ€ä¹ˆåŠï¼Ÿ
**A: å†…å­˜ä¼˜åŒ–æ–¹æ¡ˆï¼š**

1. **å‡å°‘æ‰¹å¤„ç†å¤§å°** - è®¾ç½®ä¸º2-5
2. **é™åˆ¶æœ€å¤§é¡µé¢æ•°** - åˆ†æ‰¹æ¬¡å¤„ç†
3. **ç¦ç”¨æˆªå›¾å’ŒPDF** - å‡å°‘å†…å­˜å ç”¨
4. **åŠæ—¶æ¸…ç†ç¼“å­˜** - å®šæœŸé‡å¯æœåŠ¡

## é«˜çº§åŠŸèƒ½

### 1. æ·±åº¦çˆ¬å–ç­–ç•¥

```python
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy, DFSDeepCrawlStrategy

# å¹¿åº¦ä¼˜å…ˆç­–ç•¥
bfs_strategy = BFSDeepCrawlStrategy(
    max_depth=3,
    include_external=False,
    max_pages=100
)

# æ·±åº¦ä¼˜å…ˆç­–ç•¥  
dfs_strategy = DFSDeepCrawlStrategy(
    max_depth=2,
    include_external=False,
    max_pages=50
)
```

### 2. URL è¿‡æ»¤å™¨

```python
from crawl4ai.deep_crawling.filters import DomainFilter, URLPatternFilter, FilterChain

# åŸŸåè¿‡æ»¤å™¨
domain_filter = DomainFilter(
    allowed_domains=["docs.example.com"],
    blocked_domains=["ads.example.com"]
)

# URL æ¨¡å¼è¿‡æ»¤å™¨
pattern_filter = URLPatternFilter(
    included_patterns=["*docs*", "*guide*"],
    excluded_patterns=["*.pdf", "*.jpg", "*.png"]
)

# ç»„åˆè¿‡æ»¤å™¨
filter_chain = FilterChain([domain_filter, pattern_filter])
```

### 3. å¹¶å‘æ§åˆ¶

```python
from crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher, SemaphoreDispatcher

# å†…å­˜è‡ªé€‚åº”è°ƒåº¦å™¨
memory_dispatcher = MemoryAdaptiveDispatcher(
    memory_threshold_percent=80.0,
    max_session_permit=10
)

# ä¿¡å·é‡è°ƒåº¦å™¨
semaphore_dispatcher = SemaphoreDispatcher(
    max_session_permit=5
)
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **åˆç†è®¾ç½®æ‰¹å¤„ç†å¤§å°**ï¼šæ ¹æ®ç›®æ ‡ç½‘ç«™çš„å“åº”é€Ÿåº¦è°ƒæ•´ `batch_size`
2. **ä½¿ç”¨ç¼“å­˜**ï¼šå¯¹äºé‡å¤è®¿é—®çš„é¡µé¢ï¼Œå¯ç”¨ç¼“å­˜æ¨¡å¼
3. **è¿‡æ»¤æ— æ•ˆé“¾æ¥**ï¼šä½¿ç”¨ URL è¿‡æ»¤å™¨æ’é™¤ä¸éœ€è¦çš„é¡µé¢
4. **æ§åˆ¶å¹¶å‘æ•°**ï¼šé¿å…è¿‡é«˜çš„å¹¶å‘å¯¼è‡´æœåŠ¡å™¨æ‹’ç»è®¿é—®
5. **ç›‘æ§å†…å­˜ä½¿ç”¨**ï¼šå¯¹äºå¤§è§„æ¨¡çˆ¬å–ï¼Œä½¿ç”¨å†…å­˜è‡ªé€‚åº”è°ƒåº¦å™¨

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### æ³•å¾‹å’Œé“å¾·è§„èŒƒ
| æ³¨æ„äº‹é¡¹ | è¯´æ˜ | å»ºè®®åšæ³• |
|----------|------|----------|
| **robots.txt éµå®ˆ** | æ£€æŸ¥å¹¶éµå®ˆç½‘ç«™çš„çˆ¬è™«è§„åˆ™ | è®¿é—® `/robots.txt` æŸ¥çœ‹é™åˆ¶ |
| **æœåŠ¡å™¨è´Ÿè½½** | é¿å…å¯¹ç›®æ ‡æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ› | æ§åˆ¶å¹¶å‘æ•°å’Œè¯·æ±‚é¢‘ç‡ |
| **ç‰ˆæƒä¿æŠ¤** | å°Šé‡ç½‘ç«™å†…å®¹çš„ç‰ˆæƒå’ŒçŸ¥è¯†äº§æƒ | ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç›®çš„ |
| **éšç§æ”¿ç­–** | ä¸æŠ“å–ä¸ªäººéšç§å’Œæ•æ„Ÿä¿¡æ¯ | é¿å…å¤„ç†ç”¨æˆ·æ•°æ® |
| **åˆæ³•ä½¿ç”¨** | ç¡®ä¿çˆ¬è™«è¡Œä¸ºç¬¦åˆæ³•å¾‹æ³•è§„ | äº†è§£ç›¸å…³æ³•å¾‹æ¡æ–‡ |

### æŠ€æœ¯æœ€ä½³å®è·µ
| æ–¹é¢ | å»ºè®® | åŸå›  |
|------|------|------|
| **è¯·æ±‚é¢‘ç‡** | æ¯ç§’ä¸è¶…è¿‡1-2ä¸ªè¯·æ±‚ | é¿å…è¢«è¯†åˆ«ä¸ºæ¶æ„çˆ¬è™« |
| **User-Agent** | ä½¿ç”¨çœŸå®çš„æµè§ˆå™¨æ ‡è¯† | æé«˜æˆåŠŸç‡ |
| **é”™è¯¯å¤„ç†** | å®ç°é‡è¯•å’Œé™çº§æœºåˆ¶ | æé«˜ç¨³å®šæ€§ |
| **æ•°æ®éªŒè¯** | éªŒè¯æŠ“å–æ•°æ®çš„å®Œæ•´æ€§ | ç¡®ä¿æ•°æ®è´¨é‡ |
| **ç›‘æ§æ—¥å¿—** | è®°å½•è¯¦ç»†çš„æ“ä½œæ—¥å¿— | ä¾¿äºé—®é¢˜æ’æŸ¥ |

### åçˆ¬è™«åº”å¯¹ç­–ç•¥
| åçˆ¬è™«æœºåˆ¶ | åº”å¯¹æ–¹æ³• | å®ç°æ–¹å¼ |
|------------|----------|----------|
| **IPé™åˆ¶** | ä½¿ç”¨ä»£ç†è½®æ¢ | é…ç½®å¤šä¸ªä»£ç†æœåŠ¡å™¨ |
| **éªŒè¯ç ** | äººå·¥å¤„ç†æˆ–OCR | æš‚åœä»»åŠ¡ç­‰å¾…å¤„ç† |
| **JSæ¸²æŸ“** | å¯ç”¨æµè§ˆå™¨æ¸²æŸ“ | ä½¿ç”¨Playwrightå¼•æ“ |
| **åŠ¨æ€Token** | åˆ†æè¯·æ±‚æµç¨‹ | æ¨¡æ‹Ÿå®Œæ•´çš„ç”¨æˆ·è¡Œä¸º |
| **è¡Œä¸ºæ£€æµ‹** | æ¨¡æ‹Ÿäººç±»è¡Œä¸º | éšæœºå»¶è¿Ÿå’Œè·¯å¾„ |

## ğŸš€ é¡¹ç›®è·¯çº¿å›¾

### å·²å®ŒæˆåŠŸèƒ½ âœ…
- [x] Webå¯è§†åŒ–ç•Œé¢
- [x] å¤šç§çˆ¬å–ç­–ç•¥ï¼ˆBFS/DFSï¼‰
- [x] å®æ—¶è¿›åº¦æ˜¾ç¤º
- [x] å¤šæ ¼å¼è¾“å‡ºæ”¯æŒ
- [x] LLMæ™ºèƒ½æå–
- [x] è¿‡æ»¤å™¨å’Œé…ç½®ç³»ç»Ÿ
- [x] é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

### è®¡åˆ’ä¸­åŠŸèƒ½ ğŸ”„
- [ ] åˆ†å¸ƒå¼çˆ¬å–æ”¯æŒ
- [ ] æ›´å¤šLLMæä¾›å•†é›†æˆ
- [ ] å¯è§†åŒ–æ•°æ®åˆ†æé¢æ¿
- [ ] çˆ¬è™«ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿ
- [ ] äº‘ç«¯éƒ¨ç½²æ”¯æŒ
- [ ] APIæ¥å£å¼€æ”¾

### é•¿æœŸè§„åˆ’ ğŸ“‹
- [ ] æœºå™¨å­¦ä¹ å†…å®¹åˆ†ç±»
- [ ] è‡ªåŠ¨åŒ–æµ‹è¯•è¦†ç›–
- [ ] æ€§èƒ½ç›‘æ§é¢æ¿
- [ ] å¤šè¯­è¨€ç•Œé¢æ”¯æŒ
- [ ] ä¼ä¸šçº§åŠŸèƒ½æ‰©å±•

## ğŸ¤ è´¡çŒ®æŒ‡å—

### å¦‚ä½•è´¡çŒ®
1. **æŠ¥å‘Šé—®é¢˜**
   - åœ¨GitHub Issuesä¸­è¯¦ç»†æè¿°é—®é¢˜
   - æä¾›å¤ç°æ­¥éª¤å’Œç¯å¢ƒä¿¡æ¯
   - é™„ä¸Šç›¸å…³æ—¥å¿—å’Œæˆªå›¾

2. **æäº¤ä»£ç **
   - Forké¡¹ç›®åˆ°æ‚¨çš„GitHub
   - åˆ›å»ºåŠŸèƒ½åˆ†æ”¯è¿›è¡Œå¼€å‘
   - ç¡®ä¿ä»£ç é€šè¿‡æµ‹è¯•
   - æäº¤Pull Request

3. **å®Œå–„æ–‡æ¡£**
   - æ›´æ–°READMEå’Œä½¿ç”¨è¯´æ˜
   - æ·»åŠ ä»£ç æ³¨é‡Šå’Œç¤ºä¾‹
   - ç¿»è¯‘å¤šè¯­è¨€æ–‡æ¡£

4. **åˆ†äº«ç»éªŒ**
   - åœ¨Issuesä¸­åˆ†äº«ä½¿ç”¨æŠ€å·§
   - è´¡çŒ®æ›´å¤šç½‘ç«™çš„æŠ“å–é…ç½®
   - å‚ä¸ç¤¾åŒºè®¨è®º

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/your-repo/web-crawler.git
cd web-crawler

# 2. å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# 3. è¿è¡Œæµ‹è¯•
python -m pytest tests/

# 4. å¯åŠ¨å¼€å‘æœåŠ¡å™¨
cd web && python start.py
```

## ğŸ“ æ”¯æŒä¸è”ç³»

### è·å–å¸®åŠ©
- **ğŸ“– æ–‡æ¡£**: æŸ¥çœ‹å®Œæ•´çš„ä½¿ç”¨æ–‡æ¡£
- **ğŸ’¬ è®¨è®º**: åœ¨GitHub Discussionsä¸­æé—®
- **ğŸ› æŠ¥å‘Š**: åœ¨GitHub Issuesä¸­æŠ¥å‘Šbug
- **ğŸ“§ è”ç³»**: é€šè¿‡é‚®ä»¶è”ç³»ç»´æŠ¤å›¢é˜Ÿ

### ç¤¾åŒºèµ„æº
- **ç¤ºä¾‹é…ç½®**: æŸ¥çœ‹ `examples/` ç›®å½•
- **è§†é¢‘æ•™ç¨‹**: è§‚çœ‹æ“ä½œæ¼”ç¤ºè§†é¢‘
- **æœ€ä½³å®è·µ**: é˜…è¯»ç¤¾åŒºåˆ†äº«çš„ç»éªŒ
- **æ›´æ–°æ—¥å¿—**: å…³æ³¨ç‰ˆæœ¬æ›´æ–°åŠ¨æ€

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº **MIT è®¸å¯è¯** å¼€æºï¼Œè¿™æ„å‘³ç€ï¼š

âœ… **å…è®¸çš„ä½¿ç”¨æ–¹å¼**
- å•†ä¸šä½¿ç”¨
- ä¿®æ”¹å’Œåˆ†å‘
- ç§äººä½¿ç”¨
- ä¸“åˆ©ä½¿ç”¨

â— **å¿…é¡»åŒ…å«çš„å†…å®¹**
- åŸå§‹è®¸å¯è¯å£°æ˜
- ç‰ˆæƒå£°æ˜

âš ï¸ **å…è´£å£°æ˜**
- è½¯ä»¶æŒ‰"åŸæ ·"æä¾›
- ä¸æä¾›ä»»ä½•å½¢å¼çš„ä¿è¯
- ä½œè€…ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»

è¯¦ç»†è®¸å¯è¯å†…å®¹è¯·æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

## ğŸŒŸ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹ä¼˜ç§€çš„å¼€æºé¡¹ç›®å’Œè´¡çŒ®è€…ï¼š

- **[Crawl4AI](https://github.com/unclecode/crawl4ai)** - å¼ºå¤§çš„AIé©±åŠ¨ç½‘é¡µçˆ¬è™«æ¡†æ¶
- **[Playwright](https://playwright.dev/)** - ç°ä»£åŒ–çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·
- **[Flask](https://flask.palletsprojects.com/)** - è½»é‡çº§Webæ¡†æ¶
- **[Socket.IO](https://socket.io/)** - å®æ—¶é€šä¿¡åº“

ç‰¹åˆ«æ„Ÿè°¢æ‰€æœ‰æäº¤Issueã€PRå’Œå»ºè®®çš„ç¤¾åŒºè´¡çŒ®è€…ï¼ğŸ™

---

<div align="center">
  <p>å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª â­ Starï¼</p>
  <p>è®©æ›´å¤šäººå‘ç°è¿™ä¸ªæœ‰ç”¨çš„å·¥å…· ğŸš€</p>
</div> 